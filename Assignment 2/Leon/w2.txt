- DT: Is the tree a 100% accurate model of your data? If not, why not? And what are the consequences?
No, the tree is not a 100% accurate model of the data. For example, one leaf is not pure and can lead to both a yes and a no given the colour yellow and a round shape.

- DT: What is pruning, and what is it used for?
Pruning is synonymous for reducing the size of the tree in a bottum-up fashion. Pruning a tree can help to make generalisation possible, limiting the risks of overfitting. It can also improve the performance of a system, since the task is limited and the amount of leaves is reduced.

- DT: What parameters can you change in sklearn that will affect the structure of a tree? What do they do?
Both the maximum number of leaves (in sklearn: max_leaf_nodes) and the minimum number of samples per leaf (in sklearn: min_samples_leaf) can affect the structure of a tree. max_leaf_nodes prevents a tree from growing to infinity

- DT: By changing such parameters, do results change? Describe how they do, in case.
We changed the parameter values for max_leaf_nodes and min_samples_leaf dynamically by using GridSearchCV, in order to find the best parameter combination. For max_leaf_nodes, we tested with the values [None, 75, 65, 60, 50] and for min_samples_leaf we tested with the values [1, 2, 3, 0.1, 0.2]. The best parameter combination appeared to be {'clf__max_leaf_nodes': 50, 'clf__min_samples_leaf': 3} for the given parameter values, using a weighted F1-score as scoring parameter. Other parameter combinations tended to achieve lower F1-scores.

- KNN: Is acuracy better with a lower of higher K?
Using the same GridSearchCV setup as in the previous question, we can conclude that accuracy is at its best when K it has a rather higher value. This has been tested with parameter values [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27]. In this case, we consider 15 a high(er) value (contrary to the usage of n_neighbors < 10.

- KNN: Does class performance change substantially with varying values of K?
Yes and no. For our given test range of K, we do see differences in performances per class. However, we cannot assume these changes to be substantial or significant really.

- KNN: How does changing K affect the bias/variance trade off?
KNN is known for having high variance and low bias. Increasing K can lead to an increasing bias, as the system will act less 'static' and allow for more different target values when used with test data. However, a higher K might also lead to decreasing variance as the error for test data might increase. It is vital to find the optimal K, not just the highest K.

- Comparison: Compare the time it takes to train and test a KNN, a DT and a Naive Bayes, and comment on the differences.
The times are as follows:
- KNN = 37,54 seconds
- DT = 83,78 seconds
- NB = 0.48 seconds
There is a huge difference between Naive Bayes and the other classification paradigms! We can assume that Naive Bayes can make up for its reputation when it comes to being a fast classification algorithm. The other algorithms are known for 'taking their time' in either constructing complex trees or by computing beyond storing samples. KNN is (thus) regarded as lazy.

- What is your best model? Please specify what algorithm and what features you have eventually chosen, and why?

